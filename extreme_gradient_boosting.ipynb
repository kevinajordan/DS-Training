{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "extreme_gradient_boosting.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinajordan/DS-Training/blob/master/extreme_gradient_boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJUCZz67IzSB",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Boosting\n",
        "\n",
        "Helpful Articles and Videos:\n",
        "* http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n",
        "* https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d\n",
        "* Gradient Boosting for Regression: https://www.youtube.com/watch?v=3CC4N4z3GJc\n",
        "* Gradient Boosting for Classification: https://www.youtube.com/watch?v=jxuNLH5dXCs&t=824s\n",
        "\n",
        "Gradient boosting involves three elements:\n",
        "1. A loss function to be optimized.\n",
        "* it is a generic enough framework that any **diﬀerentiable** loss function can be used.\n",
        "\n",
        "2. A weak learner to make predictions.\n",
        "* Decision trees are used as the weak learner in gradient boosting. Speciﬁcally regression trees are used that output real values for splits and whose output can be added together, allowing subsequent model outputs to be added and correct the residuals in the predictions. \n",
        "\n",
        "3. An additive model to add weak learners to minimize the loss function.\n",
        "\n",
        "* Trees are added one at a time, and existing trees in the model are not changed. A gradient descent procedure is used to minimize the loss when adding trees.  Instead of parameters, we have weak learner sub-models or more speciﬁcally decision trees. After calculating the loss, to perform the gradient descent procedure, we must add a tree to the model that reduces the loss (i.e. follow the gradient). We do this by parameterizing the tree, then modify the parameters of the tree and move in the right direction by reducing the residual loss. Generally this approach is called functional gradient descent or gradient descent with functions.\n",
        "The output for the new tree is then added to the output of the existing sequence of trees in an eﬀort to correct or improve the ﬁnal output of the model. A ﬁxed number of trees are added or training stops once loss reaches an acceptable level or no longer improves on an external validation dataset.\n",
        "\n",
        "# XGBoost\n",
        "\n",
        "XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.\n",
        "\n",
        "The two reasons to use XGBoost are also the two goals of the project:\n",
        "1. Execution Speed.\n",
        "2. Model Performance.\n",
        "\n",
        "Check this link to see benchmark performance for several ML algorithms:\n",
        "https://github.com/szilard/benchm-ml\n",
        "\n",
        "Dataset: Pima Indians Diabetes Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S27SngQ1D7YO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone DS-Training repo for datasets and skeleton code\n",
        "!git clone https://github.com/kevinajordan/DS-Training.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLyEzFtFEAI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set our working directory to the dataset folder\n",
        "import os\n",
        "os.chdir('DS-Training/datasets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBleIxWLEMPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMd7BeFBIsIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQhjyLwlDjHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data with pandas\n",
        "pima = pd.read_csv('diabetes.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qOiByvqHrpk",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning\n",
        "\n",
        "Insert some cells below and clean your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI1oA4OfDnuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split data into X and y \n",
        "feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age','Glucose','BloodPressure','DiabetesPedigreeFunction']\n",
        "X = pima[feature_cols] # Features\n",
        "y = pima.Outcome # Target variable\n",
        "\n",
        "# split data into train and test sets  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1sikZ8MElNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBClassifier \n",
        "\n",
        "model = XGBClassifier() \n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(model)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zu2TO5sE2SG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make predictions for test data \n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# evaluate predictions \n",
        "accuracy = accuracy_score(y_test, predictions) \n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lnAlGgIFVVw",
        "colab_type": "text"
      },
      "source": [
        "# K-fold Cross-Validation\n",
        "Cross-validation is an approach that you can use to estimate the performance of a machine learning algorithm with less variance than a single train-test set split. It works by splitting the dataset into k-parts (e.g. k = 5 or k = 10). Each split of the data is called a fold. The algorithm is trained on k−1 folds with one held back and tested on the held back fold. This is repeated so that each fold of the dataset is given a chance to be the held back test set. After running cross-validation you end up with k-diﬀerent performance scores that you can summarize using a mean and a standard deviation. \n",
        "\n",
        "The result is a more reliable estimate of the performance of the algorithm on new data given your test data. It is more accurate because the algorithm is trained and evaluated multiple times on diﬀerent data.\n",
        "\n",
        "We can use k-fold cross-validation support provided in scikit-learn. First we must create the KFold object specifying the number of folds and the size of the dataset. We can then use this scheme with the speciﬁc dataset. The cross val score() function from scikit-learn allows us to evaluate a model using the cross-validation scheme and returns a list of the scores for each model trained on each fold.\n",
        "\n",
        "For modest sized datasets in the thousands or tens of thousands of observations, k values of 3, 5 and 10 are common. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Raec7UzEF8Nt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold \n",
        "from sklearn.model_selection import cross_val_score "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxnPfxmsGEWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CV model \n",
        "model = XGBClassifier() \n",
        "kfold = KFold(n_splits=10, random_state=7) \n",
        "results = cross_val_score(model, X, y, cv=kfold) \n",
        "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "744oBH76IeyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}