{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro Neural Networks_part1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinajordan/DS-Training/blob/master/Intro_Neural_Networks_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a_e_EFwjQb_",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Neural Networks\n",
        "![alt text](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41377-019-0151-0/MediaObjects/41377_2019_151_Fig2_HTML.png?as=webp)\n",
        "\n",
        "*Note: This section borrows heavily from Dr. Roysdons \"A Pracitioners Guide to Neural Networks\".*\n",
        "\n",
        "### Nodes\n",
        "Analogous to biological neurons, ANN’s have a similar structure with\n",
        "the output of one neuron connected to the input of another neuron. We\n",
        "represent these networks as connected layers of nodes, where each node\n",
        "has many “weighted” inputs from previous nodes[2]. The output of each node is the result of an activation function applied to each input, followed by the sum of the activation function’s, multiplied by the weight of that node.\n",
        "\n",
        "mathematically, this looks like:\n",
        "\n",
        "$x_1 w_1 + x_2 w_2 + x_3 w_3 + b $\n",
        "\n",
        "* w is the weights\n",
        "* b is the bias\n",
        "* x is the inputs\n",
        "\n",
        "### Activation functions\n",
        "If the input is above a user defined threshold, the activation function switches state, e.g. from 0 to 1, -1 to 1, or from 0 to > 0. A common activation function is the sigmoid function.\n",
        "\n",
        "### Bias \n",
        "By adding a bias term, you can make the node simulate a\n",
        "generic *if* function, i.e. if (x > z) then 1 else 0. Without a bias term,\n",
        "you are unable to vary z, and the function will be always near 0. This is\n",
        "simple example demonstrates the need for a bias to simulate conditional\n",
        "relationships.\n",
        "\n",
        "## Structure of a Neural Network\n",
        "\n",
        "An artifical neural network consists of:\n",
        "* an input layer\n",
        "* a hidden layer(s)\n",
        "* and an output layer - this produces the result of a neural network; the prediction.\n",
        "\n",
        "\n",
        "## Feed Forward Process\n",
        "\n",
        "Succintly, feedforward artificial neural networks are networks where the ouput from one node feeds into a node in the next layer and so on, until the ouput layer produces the result. The ouput is not fed back into the model to update the weights.\n",
        "\n",
        "This is the most basic neural network. Each input shown to a feedforward neural network is processed independently, with no state kept between inputs. With these networks, in order to process a sequence or a temporal series of data points, you have to show the entire sequence to the network at once: turn it into a single data point. For instance, with the IMDB movie review dataset, an entire movie review is transformed into a single large vector and processed in one go.\n",
        "\n",
        "We will demonstrate this in python later.\n",
        "\n",
        "## Gradient Descent\n",
        "\n",
        "Find which weights and bias's minimize a cost function.\n",
        "\n",
        "![alt text](https://saugatbhattarai.com.np/wp-content/uploads/2018/06/gradient-descent-1.jpg)\n",
        "\n",
        "\n",
        "## Problem: Classify Handwritten Digits\n",
        "\n",
        "Classify grayscale images of handwritten digits (28 x 28 pixels) into their 10 categories (0 through 9). Solving MNIST is like the \"Hello World\" of deep learning.\n",
        "\n",
        "![alt text](https://3.bp.blogspot.com/-mDyzBzA4btg/V4_Z0f2mc7I/AAAAAAAAE3M/dtU8hT661fQWtnRC_JvIH_4qifQomZ4PACLcB/s1600/MNIST_neuralnet_image.png)\n",
        "\n",
        "### Dataset: MNIST\n",
        "Set of 60,000 training images and 10,000 test images. Assembled by NIST in the 1980's.\n",
        "\n",
        "#### Sources used for this training:\n",
        "\n",
        "1. Deep Learning with Python by Francois Chollet\n",
        "2. A Practioners Guide to Neural Networks by P. F. Roysdon, PhD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgnA9a5fjGFf",
        "colab_type": "code",
        "outputId": "f586f31b-5f5f-43a8-ce73-036255c880d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Load MNIST\n",
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1iKV5xIsUnI",
        "colab_type": "code",
        "outputId": "17a4eacf-4a75-4a3f-b916-7834434e7444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dAuETOqx80K",
        "colab_type": "code",
        "outputId": "6afbc002-690f-4cb9-8c84-7cfbb644bfea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCjtf0b2yAra",
        "colab_type": "code",
        "outputId": "4a5125da-d0be-4ff5-d818-8559c848dece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43SpAn-pyGbD",
        "colab_type": "code",
        "outputId": "566fe642-1219-492f-d5c2-d17da3e2a495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq-ULJn1yLxz",
        "colab_type": "code",
        "outputId": "25cd0f58-19ab-46be-b762-1593d4a67a69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycnqMj7wyQ5x",
        "colab_type": "code",
        "outputId": "1b209cfc-9d5b-4f21-e27c-9d501a4a1f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_labels"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhVqPCNDybdY",
        "colab_type": "text"
      },
      "source": [
        "## Initial Network Architecture\n",
        "\n",
        "The core building block of neural networks is the *layer*, a data-processing module that you can think of as a filter for data. Some data goes in, and it comes out in a more useful form. \n",
        "\n",
        "Specifically, layers extract *representations* out of the data fed into them - hopefully, representations that are more meaningful for the problem at hand.\n",
        "\n",
        "A deep learning model is like a sieve for data processing, made of a succession of increasingly refined data filters - the layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64JIjy3eyTPf",
        "colab_type": "code",
        "outputId": "6cd4cc1a-2416-46ce-c7ae-92d22cb93267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# The network architecture\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0813 21:15:13.182682 140217238472576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0813 21:15:13.254055 140217238472576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0813 21:15:13.263946 140217238472576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ3SGUaXy7nB",
        "colab_type": "text"
      },
      "source": [
        "Here, our network consists of a sequence of two Dense layers, which are densely connected (aka *fully connected*) neural layers. \n",
        "\n",
        "The second (and last) layer is a 10-way softmax layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 classes.\n",
        "\n",
        "We need to pick 3 more things to make our network ready to train:\n",
        "* A *loss* function\n",
        "* An *optimizer* - the mechanism through which the network will update itself based on the data it sees and its loss function.\n",
        "* *Evaluation Metrics* - here, we'll only care about accuracy. As a rule of thumb, it's always best to pick one metric to optimize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yU7WT5L1FLh",
        "colab_type": "code",
        "outputId": "5e1fd99f-8c29-4397-8737-867926bb4e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Compilation step\n",
        "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0813 21:15:19.865456 140217238472576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0813 21:15:19.906214 140217238472576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dumt9TLd1fdm",
        "colab_type": "text"
      },
      "source": [
        "Before training, we'll preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the [0, 1] interval.\n",
        "\n",
        "Previously, our training images, for instance, were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape (60000, 28* 28) with values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PalemyO1aZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing the image data\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape(10000, 28 * 28)\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bfmipDP485B",
        "colab_type": "text"
      },
      "source": [
        "We also need to categorically encode the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0GxYKF35CYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing the labels\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDSN-zLz5TEk",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to train our network. This is done by the same method we have seen with scikit-learn: *fit*. We *fit* the model to it's training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etvstqDa48NE",
        "colab_type": "code",
        "outputId": "65d89a62-6720-4fc9-bdf3-5f79d0ac2b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "network.fit(train_images, train_labels, epochs = 5, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0813 21:33:21.088517 140217238472576 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0813 21:33:21.161081 140217238472576 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 6s 99us/step - loss: 0.2576 - acc: 0.9256\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.1038 - acc: 0.9695\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0687 - acc: 0.9795\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0498 - acc: 0.9853\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0374 - acc: 0.9891\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f86a4650160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGgfz35C54oD",
        "colab_type": "text"
      },
      "source": [
        "Two quantities are displayed during the training: the loss of the network over the training data, and the accuracy of the network over the training data.\n",
        "\n",
        "Quickly, we reached an accuracy of 98.9% on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM3R8Mky5lOi",
        "colab_type": "code",
        "outputId": "fa2515b5-f5ea-4f56-e209-552c83873d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "print ('test accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 52us/step\n",
            "test accuracy: 0.9792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldTXxeRF6d4P",
        "colab_type": "text"
      },
      "source": [
        "This gap between the training accuracy and test accuracy is an example of *overfitting*. The fact that machine-learning methods tend to perform worse on new data than on their training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYtYW7E-DC6j",
        "colab_type": "text"
      },
      "source": [
        "## Data Representations for Neural Networks\n",
        "\n",
        "In general, all current machine-learning systems use tensors as their basic data structure. They are fundamental to the field of machine learning and deep learning. \n",
        "\n",
        "Tensors are a generalization of matrices to an arbitrary number of dimensions (remember a *dimension* is often called an *axis*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awruMxSHD3JG",
        "colab_type": "text"
      },
      "source": [
        "### Scalars (0D Tensors)\n",
        "\n",
        "A tensor that only contains one number is called a *scalar*. In Numpy, a float32 or float64 number is a scalar tensor.\n",
        "\n",
        "The number of axes of a tensor is also called its rank.\n",
        "\n",
        "You can display the number of axes of a Numpy tensor via the ndim attribute. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48RqS4wa6bih",
        "colab_type": "code",
        "outputId": "f93f071d-d09b-4141-a641-dcea8f8e5089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "x = np.array(12)\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCpyAxv2EaIz",
        "colab_type": "code",
        "outputId": "f6044027-16ee-49e3-b3ae-962407ad2b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.ndim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krdUC7QqEdOQ",
        "colab_type": "text"
      },
      "source": [
        "### Vectors (1D tensors)\n",
        "An array of numbers is called a vector, or 1D tensor. A vector has exactly 1 axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq_WYHO6Ebgn",
        "colab_type": "code",
        "outputId": "03d29bf4-59f8-47c3-d5ea-d860461ab4a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = np.array([12, 3, 6, 14])\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12,  3,  6, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP_xIxKPEujk",
        "colab_type": "code",
        "outputId": "1449b0f9-8efd-4418-c965-6fd988e6b303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.ndim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZPMcPUbEyAe",
        "colab_type": "text"
      },
      "source": [
        "This vector has 5 entries and so is called a *5-dimensional* vector.\n",
        "\n",
        "A 5D vector has only 5 dimensions along it's axis, where a 5D tensor has five axes.\n",
        "\n",
        "Dimensionality can denote either the number of entries along a specific axis (as in the case of our 5D vector) or the number of axes in a tensor (such as a 5D tensor). This can be confusing. For tensors, its technically more correct to talk about a *tensor of rank 5*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MohZHhrUFjBr",
        "colab_type": "text"
      },
      "source": [
        "### Matrices (2D tensors)\n",
        "An array of vectors is a matrix, or 2D tensor. A matrix has two axes (referred to as *rows* and *columns*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSdctnCKEvzE",
        "colab_type": "code",
        "outputId": "3ea4653a-b5d6-4618-b3fb-77b9366bd43a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = np.array([[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]])\n",
        "x.ndim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66surgVJGSTG",
        "colab_type": "text"
      },
      "source": [
        "### 3D tensors and higher-dimensional tensors\n",
        "if you pack such matrices in a new array, you obtain a 3D tensor, which visually you can think of as a cube of numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSMBItjkGQD4",
        "colab_type": "code",
        "outputId": "8712c15e-a42c-440b-8b5a-37fce16cb2e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = np.array([[[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2],\n",
        "              [5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2],\n",
        "              [5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [7, 80, 4, 36, 2]]])\n",
        "x.ndim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rhTfecjG7ly",
        "colab_type": "text"
      },
      "source": [
        "In deep learning, you'll generally manipulate tensors that are 0D to 4D, although you may go up to 5D if you process video data.\n",
        "\n",
        "### Key Attributes of Tensors\n",
        "\n",
        "* Number of axes (rank)\n",
        "* Shape - A tuple of integers that describes how many dimensions the tensor has along each axis. For instance, the previous matrix example has shape (3, 5), and the 3D tensor example has shape (3, 3, 5).\n",
        "* Data Type ( *dtype* in python libraries) - type of data contained in a tensor; for instance, a tensor's type could be float32, uint8, float64, and so on. Note: string tensors don't exist in Numpy (or most other libraries). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwMjaMNSG1SC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6a9a4fc5-e743-4e13-8cab-ca37c5b9cbe9"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47L8jOI1XOhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c06e29a-2761-4ee9-a1f7-a5be98815906"
      },
      "source": [
        "print(train_images.ndim)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t-dV4D7Xfx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c69955a1-d59b-4afc-f4ce-ccf754bd66c5"
      },
      "source": [
        "print(train_images.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6ngqwp3XkiJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f0b0536-24d1-467c-ac46-8bc5f74c61c3"
      },
      "source": [
        "print(train_images.dtype)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uint8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2UvGBYdXndt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "7258a34d-36c6-475e-fc0f-dfe58510e884"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "digit = train_images[4]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBEX4gaAIPxAU4QeCIvxAUP8HF8NDxhA0MHUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-dDjmRuX8qh",
        "colab_type": "text"
      },
      "source": [
        "### Manipulating Tensors in Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VpiEEUrX3lh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5eeee27b-1563-4163-e4c2-32f025107844"
      },
      "source": [
        "# tensor slicing\n",
        "my_slice = train_images[10:100] # selects digits 10 to 99 (100 is not included)\n",
        "print(my_slice.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(90, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7vTO5-NYNt7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef628457-afc0-414c-c75c-af164203a85a"
      },
      "source": [
        "# more detailed for each axis of a tensor\n",
        "my_slice = train_images[10:100, :, :] # equivalent to previous example\n",
        "my_slice.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4WLsAgpYk3L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b482a47c-1054-4e69-a704-2446b784f438"
      },
      "source": [
        "my_slice = train_images[10:100, 0:28, 0:28] # also equivalent to previous example\n",
        "my_slice.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciZ2avRcYmbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_slice = train_images[:. 14:, 14:] # selects 14 x 14 pixels in the bottom right corner of all images\n",
        "my_slice = train_images[:, 7:-7, 7:-7] # select 14x14 pixes in the middle of all images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSvgZ4o3aKw-",
        "colab_type": "text"
      },
      "source": [
        "In general, the first axis in all data tensors you'll come across in deep learning will be the *sample* axis (sometimes called the *samples* dimension). For our example, samples are images of digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaG-UXhTahEz",
        "colab_type": "text"
      },
      "source": [
        "### Notion of Data Batches\n",
        "Deep learning models don't process an entire dataset at once; rather, they break data into small batches. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XswycmO4azxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = train_images[:128] # first batch\n",
        "next_batch = train_images[128:256] # second batch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1v58eSbbXDp",
        "colab_type": "text"
      },
      "source": [
        "### Real-World Examples of Data Tensors\n",
        "* *Vector data* - 2D tensors of shape (samples, features)\n",
        "* *Timeseries data or sequence data* - 3D tensors of shape (samples, timesteps, features)\n",
        "* *Images*  - 4D tensors of shape (samples, height, width, channels) or (samples, channels, height, width)\n",
        "* *Video* - 5D tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "465OuTsFcMP2",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Operations - The Gears of Neural Networks\n",
        "\n",
        "All transformations learned by neural networks can be reduced to a handful of tensor operations applied to tensors of numeric data. Let's return to our neural network we built above. \n",
        "\n",
        "We were building our network by stacking Dense layers on top of each other. A single Keras layer looks like this:\n",
        "\n",
        "\n",
        "```\n",
        "keras.layers.Dense(512, activation='relu')\n",
        "```\n",
        "This layer can be interpreted as a function, which takes as input a 2D tensor and returns another 2D tensor - a new representation for the input tensor. Specifically, the function is as follows (where W is a 2D tensor and b is a vector, both attributes of the layer):\n",
        "```\n",
        "output = relu(dot(W, input) + b)\n",
        "```\n",
        "\n",
        "Let's unpack this. We have three tensor operations here:\n",
        "* a dot product (dot) between the input tensor and a tensor named W\n",
        "* an addition (+) between the resulting 2D tensor and a vector b \n",
        "* a relu operation. relu(x) is max(x, 0).\n",
        "\n",
        "All of these operations deal with Linear Algebra expressions. Instead of covering the mathematical notation, we will cover these operations with short python snippets so they are easier to understand for those who have no mathematical background.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Edu0YI1hVeV",
        "colab_type": "text"
      },
      "source": [
        "### Element-Wise Operations\n",
        "\n",
        "The relu operation and addition are *element-wise* operations: operations that are applied independently to each entry in the tensors being considered. This means these operations are highly amenable to massively parallel implementations.\n",
        "\n",
        "If you want to write a naive Python implementation of an element-wise operation, you use a *for* loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa2G93W4h7QA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# naive implementation of an element-wise relu operation\n",
        "def naive_relu(x):\n",
        "  assert len(x.shape) == 2 # x is a 2D Numpy tensor.\n",
        "  x = x.copy() # avoid overwriting the input tensor\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i, j] = max(x[i, j], 0)\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X5t980vifsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# naive addition\n",
        "def naive_add(x, y):\n",
        "  assert len(x.shape) == 2\n",
        "  assert x.shape == y.shape\n",
        "\n",
        "  x = x.copy() # avoid overwriting the input tensor\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(x.shape[1]):\n",
        "      x[i, j] += y[i, j]\n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkZ5yNbijM8l",
        "colab_type": "text"
      },
      "source": [
        "The same functions defined above, implemented in Numpy very efficiently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxb2_njmjLxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "z = x + y # element wise addition\n",
        "z = np.maximum(z, 0.) # element wise relu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCWD1MyUjhlu",
        "colab_type": "text"
      },
      "source": [
        "### Broadcasting\n",
        "\n",
        "What happens with addition when the shapes of two tensors being added, differ?\n",
        "\n",
        "When possible, the smaller tensor will be broadcasted to match the shape of the larger tensor. Broadcasting consists of two steps:\n",
        "1. Axes are added to the smaller tensor to match the ndim of the larger tensor.\n",
        "2. The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor.\n",
        "\n",
        "Consider X with shape (32, 10) and y with shape (10, ). First, we add an empty first axis to y, whose shape becomes (1, 10). Then, we repeat y 32 times alongside this new axis, so that we end up with a tensor Y with shape (32, 10), where Y[i, :] == y for i in range(0, 32). At this point we can proceed to add X and Y, because they have the same shape.\n",
        "\n",
        "Let's look at the following implementation of broadcasting in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50ysEWZjlWTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example applies the element wise maximum operation to two tensors of different shapes via broadcasting\n",
        "import numpy as np\n",
        "x = np.random.random((64, 3, 32, 10)) # random tensor with shape (64, 3, 32, 10)\n",
        "y = np.random.random((32, 10)) # random tensor with shape (32, 10)\n",
        "\n",
        "z = np.maximum(x, y) # output z has shape (64, 3, 32, 10) like x."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGTXdPn8l7Hq",
        "colab_type": "text"
      },
      "source": [
        "### Tensor Dot\n",
        "\n",
        "The dot operation, also called a *tensor product* (not element-wise product), is the most common, most useful tensor operation. Contrary to element-wise operations, it combines entries in the input tensors.\n",
        "\n",
        "An element-wise product is done with the * operator in Numpy, Keras, Theano, and TensorFlow. *dot* uses a different syntax in TensorFlow, but in both Numpy and Keras it's done using the standard *dot* operator.\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "z = np.dot(x, y)\n",
        "```\n",
        "The dot product between two vectors is a scalar and that only vectors with the same number of elements are compatible for a dot product. \n",
        "\n",
        "You can take the dot product of two matrices x and y (dot(x, y)) if and only if x.shape[1] == y.shape[0]. The result is a matrix with shape (x.shape[0], y.shape[1]), where the coefficients are the vector products between the rows of x and the columns of y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKbDwqK6lq5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naive_matrix_dot(x, y):\n",
        "  assert len(x.shape) == 2\n",
        "  assert len(y.shape) == 2\n",
        "  assert x.shape[1] = y.shape[0] # the first dimension of x must be the same as the 0th dimension of y\n",
        "\n",
        "  z = np.zeros((x.shape[0],. y.shape[1]))\n",
        "  for i in range(x.shape[0]):\n",
        "    for j in range(y.shape[1]):\n",
        "      row_x = x[i, :]\n",
        "      column_y = y[:, j]\n",
        "      z[i, j] = naive_vector_dot(row_x, column_y)\n",
        "  return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr_UPDa0I-AK",
        "colab_type": "text"
      },
      "source": [
        "## Tensor reshaping\n",
        "\n",
        "A third type of tensor operation that's essential to understand is tensor reshaping. We used it previously when we preprocessed the digits data before feeding it into our network:\n",
        "```\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "```\n",
        "\n",
        "Reshaping a tensor means rearranging its rows and columns to match a target shape. Naturally, the reshaped tensor has the same total number of coefficients as the initial tensor. \n",
        "\n",
        "Below are some simple examples:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIkAVW1sJp6C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bcc2a52c-3aa3-4db8-83c7-eff15e08497c"
      },
      "source": [
        "x = np.array([[0., 1.], [2., 3.], [4., 5.]])\n",
        "print(x.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVFI0Z-qJza0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "decb9445-efdb-4606-c0d6-827861f64942"
      },
      "source": [
        "x = x.reshape((6, 1))\n",
        "x"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [1.],\n",
              "       [2.],\n",
              "       [3.],\n",
              "       [4.],\n",
              "       [5.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqAML7QWJ4bH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7f47e5da-7e42-4a5e-f367-a0833b545c84"
      },
      "source": [
        "x = x.reshape((2, 3))\n",
        "x"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 2.],\n",
              "       [3., 4., 5.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9wvbdgQKLST",
        "colab_type": "text"
      },
      "source": [
        "A common case of reshaping is *transposition*. Transposing a matrix means exchanging its rows and its columns, so that x[i, :] becomes x[:, i]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBJ1lmU3KGeu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80c3074c-5e36-4a51-98e3-4cb40818dad5"
      },
      "source": [
        "x = np.zeros((300, 20))\n",
        "x = np.transpose(x)\n",
        "print(x.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_cQTXIoKiX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiMSzZdyPD2Q",
        "colab_type": "text"
      },
      "source": [
        "## Back Propagation\n",
        "Starts with the final loss value and and works backward from the output layers to the input layer, applying the chain rule to compute the contribution that each parameter had in the loss value. It then updates the weights and biases to minimize the loss.\n",
        "\n",
        "This is already implemented for you, but it is good to know about.\n",
        "\n",
        "We will cover this more in depth in the next lesson.\n",
        "\n",
        "Helpful Video for Understanding backprop\n",
        "https://www.youtube.com/watch?v=Ilg3gGewQ5U\n"
      ]
    }
  ]
}