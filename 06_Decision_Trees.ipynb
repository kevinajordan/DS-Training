{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_Decision_Trees.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinajordan/DS-Training/blob/master/06_Decision_Trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK-EZ53_JNxG",
        "colab_type": "text"
      },
      "source": [
        "# Decision Trees\n",
        "\n",
        "![alt text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1545934190/1_r5ikdb.png)\n",
        "\n",
        "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value.\n",
        "\n",
        "Classification and Regression Trees or CART for short is an acronym introduced by Leo Breiman to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems. We will be using CART today. \n",
        "\n",
        "Task: ~ 8 min - Read the article below on CART: \n",
        "\n",
        "https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/\n",
        "\n",
        "### Short Summary of Article:\n",
        "\n",
        "Creating a binary decision tree is actually a process of dividing up the input space. A greedy approach is used to divide the space called **recursive binary splitting**. This is a numerical procedure where all the values are lined up and different split points are tried and tested using a cost function.\n",
        "\n",
        "The split with the best cost (lowest cost because we minimize cost) is selected. All input variables and all possible split points are evaluated and chosen in a greedy manner based on the cost function.\n",
        "\n",
        "* Regression: The cost function that is minimized to choose split points is the sum squared error across all training samples that fall within the rectangle.\n",
        "* Classification: The Gini cost function is used which provides an indication of how pure the nodes are, where node purity refers to how mixed the training data assigned to each node is.\n",
        "\n",
        "Splitting continues until nodes contain a minimum number of training examples or a maximum tree depth is reached.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Attribute Selection Measures\n",
        "\n",
        "Attribute selection measure is a heuristic for selecting the splitting criterion that partition data into the best possible manner.\n",
        "Most popular selection measures are Information Gain, Gain Ratio, and Gini Index.\n",
        "* Information gain computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values. Entropy can be thought of as the randomness in the data. Information gain is the decrease in entropy. Biased toward the attribute with many outcomes. Model ID3 uses information gain.\n",
        "\n",
        "* The Gain Ratio is meant to address the bias that exists in information gain. Gain ratio handles the issue of bias by normalizing the information gain using Split Info. C4.5 uses the gain ratio. \n",
        "\n",
        "![alt text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1545934190/7_xnqpo8.png)\n",
        "\n",
        "The attribute with the highest gain ratio is chosen as the splitting attribute\n",
        "\n",
        "\n",
        "* The Gini Index considers a binary split for each attribute. You can compute a weighted sum of the impurity of each partition. If a binary split on attribute A partitions data D into D1 and D2, the Gini index of D is:\n",
        "\n",
        "![alt text](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1545934191/9_atnmbc.png)\n",
        "\n",
        "The attribute with minimum Gini index is chosen as the splitting attribute.\n",
        "\n",
        "\n",
        "### HIGHLY RECOMMENDED READING:\n",
        "\n",
        "[Do we Need Hundreds of Classifiers to Solve Real World\n",
        "Classification Problems?](http://jmlr.csail.mit.edu/papers/volume15/delgado14a/delgado14a.pdf)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ1ep22XIMn8",
        "colab_type": "code",
        "outputId": "447532f2-1ab8-49fd-85fe-df846ce8de0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/kevinajordan/DS-Training.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DS-Training'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/77)   \u001b[K\rremote: Counting objects:   2% (2/77)   \u001b[K\rremote: Counting objects:   3% (3/77)   \u001b[K\rremote: Counting objects:   5% (4/77)   \u001b[K\rremote: Counting objects:   6% (5/77)   \u001b[K\rremote: Counting objects:   7% (6/77)   \u001b[K\rremote: Counting objects:   9% (7/77)   \u001b[K\rremote: Counting objects:  10% (8/77)   \u001b[K\rremote: Counting objects:  11% (9/77)   \u001b[K\rremote: Counting objects:  12% (10/77)   \u001b[K\rremote: Counting objects:  14% (11/77)   \u001b[K\rremote: Counting objects:  15% (12/77)   \u001b[K\rremote: Counting objects:  16% (13/77)   \u001b[K\rremote: Counting objects:  18% (14/77)   \u001b[K\rremote: Counting objects:  19% (15/77)   \u001b[K\rremote: Counting objects:  20% (16/77)   \u001b[K\rremote: Counting objects:  22% (17/77)   \u001b[K\rremote: Counting objects:  23% (18/77)   \u001b[K\rremote: Counting objects:  24% (19/77)   \u001b[K\rremote: Counting objects:  25% (20/77)   \u001b[K\rremote: Counting objects:  27% (21/77)   \u001b[K\rremote: Counting objects:  28% (22/77)   \u001b[K\rremote: Counting objects:  29% (23/77)   \u001b[K\rremote: Counting objects:  31% (24/77)   \u001b[K\rremote: Counting objects:  32% (25/77)   \u001b[K\rremote: Counting objects:  33% (26/77)   \u001b[K\rremote: Counting objects:  35% (27/77)   \u001b[K\rremote: Counting objects:  36% (28/77)   \u001b[K\rremote: Counting objects:  37% (29/77)   \u001b[K\rremote: Counting objects:  38% (30/77)   \u001b[K\rremote: Counting objects:  40% (31/77)   \u001b[K\rremote: Counting objects:  41% (32/77)   \u001b[K\rremote: Counting objects:  42% (33/77)   \u001b[K\rremote: Counting objects:  44% (34/77)   \u001b[K\rremote: Counting objects:  45% (35/77)   \u001b[K\rremote: Counting objects:  46% (36/77)   \u001b[K\rremote: Counting objects:  48% (37/77)   \u001b[K\rremote: Counting objects:  49% (38/77)   \u001b[K\rremote: Counting objects:  50% (39/77)   \u001b[K\rremote: Counting objects:  51% (40/77)   \u001b[K\rremote: Counting objects:  53% (41/77)   \u001b[K\rremote: Counting objects:  54% (42/77)   \u001b[K\rremote: Counting objects:  55% (43/77)   \u001b[K\rremote: Counting objects:  57% (44/77)   \u001b[K\rremote: Counting objects:  58% (45/77)   \u001b[K\rremote: Counting objects:  59% (46/77)   \u001b[K\rremote: Counting objects:  61% (47/77)   \u001b[K\rremote: Counting objects:  62% (48/77)   \u001b[K\rremote: Counting objects:  63% (49/77)   \u001b[K\rremote: Counting objects:  64% (50/77)   \u001b[K\rremote: Counting objects:  66% (51/77)   \u001b[K\rremote: Counting objects:  67% (52/77)   \u001b[K\rremote: Counting objects:  68% (53/77)   \u001b[K\rremote: Counting objects:  70% (54/77)   \u001b[K\rremote: Counting objects:  71% (55/77)   \u001b[K\rremote: Counting objects:  72% (56/77)   \u001b[K\rremote: Counting objects:  74% (57/77)   \u001b[K\rremote: Counting objects:  75% (58/77)   \u001b[K\rremote: Counting objects:  76% (59/77)   \u001b[K\rremote: Counting objects:  77% (60/77)   \u001b[K\rremote: Counting objects:  79% (61/77)   \u001b[K\rremote: Counting objects:  80% (62/77)   \u001b[K\rremote: Counting objects:  81% (63/77)   \u001b[K\rremote: Counting objects:  83% (64/77)   \u001b[K\rremote: Counting objects:  84% (65/77)   \u001b[K\rremote: Counting objects:  85% (66/77)   \u001b[K\rremote: Counting objects:  87% (67/77)   \u001b[K\rremote: Counting objects:  88% (68/77)   \u001b[K\rremote: Counting objects:  89% (69/77)   \u001b[K\rremote: Counting objects:  90% (70/77)   \u001b[K\rremote: Counting objects:  92% (71/77)   \u001b[K\rremote: Counting objects:  93% (72/77)   \u001b[K\rremote: Counting objects:  94% (73/77)   \u001b[K\rremote: Counting objects:  96% (74/77)   \u001b[K\rremote: Counting objects:  97% (75/77)   \u001b[K\rremote: Counting objects:  98% (76/77)   \u001b[K\rremote: Counting objects: 100% (77/77)   \u001b[K\rremote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 77 (delta 29), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (77/77), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EwOYoi2IYil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('DS-Training/datasets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBf6Dxr8F1Dr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5_4D_F6F5Dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJFVBIOWF57N",
        "colab_type": "code",
        "outputId": "2273ed64-0d7b-444d-87f7-d4353f124010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# look at the first few lines of your data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUwPiZJRLyvl",
        "colab_type": "text"
      },
      "source": [
        "## Data Pre-Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1SX-ByEMJiO",
        "colab_type": "text"
      },
      "source": [
        "Remove rows with null values. First mark the columns with inappropriate zeros as null and then remove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVUgyu59LxmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replace the 0's with NaN's.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ZTmMv6MVpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove the rows with NaN's."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTyzVCrmF9r4",
        "colab_type": "text"
      },
      "source": [
        "## Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNqePt3dGAF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split dataset in features and target variable\n",
        "feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age','Glucose','BloodPressure','DiabetesPedigreeFunction']\n",
        "X = pima[feature_cols] # Features\n",
        "y = pima.Outcome # Target variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCZZ4cbUGDAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhBVGz93LT7Z",
        "colab_type": "text"
      },
      "source": [
        "What sci-kit learn package implements decision trees?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef6oi7mAGFkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the right classifier\n",
        "from sklearn._____ import _________\n",
        "# Create Decision Tree classifer object\n",
        "clf = _____________()\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(X_train,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc29LuDaGHz9",
        "colab_type": "code",
        "outputId": "1065c9e8-8069-4fff-d214-482ed803fd26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics._______(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6796536796536796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "venB0BllGK0F",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing Decision Trees\n",
        "\n",
        "libraries: graphviz and pydotplus\n",
        "\n",
        "export_graphviz function converts decision tree classifier into dot file and pydotplus convert this dot file to png or displayable form on Jupyter.\n",
        "\n",
        "Read this article: https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176\n",
        "\n",
        "Now visualize your decision tree from above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gGgLYXtGTVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your decision tree following the article from above and the official documentation."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHyNEJ0iNfwm",
        "colab_type": "text"
      },
      "source": [
        "<!--\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "import pydotplus\n",
        "\n",
        "dot_data = StringIO()\n",
        "export_graphviz(clf, out_file=dot_data,  \n",
        "                filled=True, rounded=True,\n",
        "                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "graph.write_png('diabetes.png')\n",
        "Image(graph.create_png())\n",
        "\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7rOebxmGtld",
        "colab_type": "text"
      },
      "source": [
        "## Optimizing Decision Trees: Pruning\n",
        "\n",
        "* criterion : optional (default=”gini”) or Choose attribute selection measure: This parameter allows us to use the different-different attribute selection measure. Supported criteria are “gini” for the Gini index and “entropy” for the information gain.\n",
        "\n",
        "* splitter : string, optional (default=”best”) or Split Strategy: This parameter allows us to choose the split strategy. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
        "\n",
        "* max_depth : int or None, optional (default=None) or Maximum Depth of a Tree: The maximum depth of the tree. If None, then nodes are expanded until all the leaves contain less than min_samples_split samples. The higher value of maximum depth causes overfitting, and a lower value causes underfitting.\n",
        "\n",
        "**In Scikit-learn, optimization of decision tree classifier performed by only pre-pruning**. Maximum depth of the tree can be used as a control variable for pre-pruning. In the following the example, you can plot a decision tree on the same data with max_depth=3. Other than pre-pruning parameters, You can also try other attribute selection measure such as entropy.\n",
        "\n",
        "Task: ~ 10 min\n",
        "\n",
        "Try different criterion and max-depth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaOtYypkGWfS",
        "colab_type": "code",
        "outputId": "8b09c013-c4f0-4a69-a3af-37d3a2eda1aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create Decision Tree classifer object\n",
        "clf = __________(criterion=_______, max_depth=____)\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(X_train,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7705627705627706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg9G7i4pHMZF",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing Again\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS2NLz4vHPVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# display the decision tree again."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMDGfnj4N2eP",
        "colab_type": "text"
      },
      "source": [
        "<!--\n",
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus\n",
        "dot_data = StringIO()\n",
        "export_graphviz(clf, out_file=dot_data,  \n",
        "                filled=True, rounded=True,\n",
        "                special_characters=True, feature_names = feature_cols,class_names=['0','1'])\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "graph.write_png('diabetes.png')\n",
        "Image(graph.create_png())\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-MKR-EKUNsj",
        "colab_type": "text"
      },
      "source": [
        "## Implementing Decision Trees From Scratch\n",
        "\n",
        "Follow this guide and implement a decision tree from scratch:\n",
        "\n",
        "https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/\n",
        "\n",
        "Dataset: banknote\n",
        "\n",
        "http://archive.ics.uci.edu/ml/datasets/banknote+authentication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFHizp_yOl6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}