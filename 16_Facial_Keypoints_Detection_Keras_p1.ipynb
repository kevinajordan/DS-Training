{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16_Facial_Keypoints_Detection_Keras_p1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinajordan/DS-Training/blob/master/16_Facial_Keypoints_Detection_Keras_p1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-MBYxWZDl-Q",
        "colab_type": "text"
      },
      "source": [
        "# Facial Keypoint Detection\n",
        "\n",
        "Inspired by Udacity's Computer Vision Nanodegree project 1 [Udacity FKP GitHub Page](https://github.com/udacity/P1_Facial_Keypoints), with some modifications. This project will be all about defining and training a convolutional neural network to perform facial keypoint detection, and using computer vision techniques to transform images of faces.  The first step in any challenge like this will be to load and visualize the data you'll be working with. \n",
        "\n",
        "Let's take a look at an example of an image with corresponding facial keypoints.\n",
        "\n",
        "![alt text](data:image/jpeg;base64,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)\n",
        "\n",
        "Facial keypoints (also called facial landmarks) are the small magenta dots shown on each of the faces in the image above. In each training and test image, there is a single face and **68 keypoints, with coordinates (x, y), for that face**.  These keypoints mark important areas of the face: the eyes, corners of the mouth, the nose, etc. These keypoints are relevant for a variety of tasks, such as face filters, emotion recognition, pose recognition, and so on. Here they are, numbered, and you can see that specific ranges of points match different portions of the face.\n",
        "\n",
        "\n",
        "![alt text](https://lh6.googleusercontent.com/kb_PrLG_1k5EIr32SuHvSY01b5zmCrx84QQr62POFA-xas5s-NmvbQLZRAKOzMKtRcuCGl-Ha4wGJIpiR_SF8C-3QIes_IzIw2R8CVykalzkNilUXuWFuLk5Dxrqiky2xRW4LxVNCZo)\n",
        "\n",
        "\n",
        "\n",
        "In the next section let's get into the data loading and pre-processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db8O1qwZDl-R",
        "colab_type": "text"
      },
      "source": [
        "## EDA - Load, Visualize, and Understand The Data\n",
        "\n",
        "The dataset we'll be working with for this project is the [YouTube Faces Dataset](https://www.cs.tau.ac.il/~wolf/ytfaces/).\n",
        "\n",
        "You'll need to load in the images of faces and their keypoints and visualize them. These videos have been fed through some processing steps and turned into sets of image frames containing one face and the associated keypoints.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsxWOoHV-lHC",
        "colab_type": "text"
      },
      "source": [
        "#### Training and Testing Data\n",
        "\n",
        "This facial keypoints dataset consists of 5770 color images. All of these images are separated into either a training or a test set of data.\n",
        "\n",
        "* 3462 of these images are training images, for you to use as you create a model to predict keypoints.\n",
        "* 2308 are test images, which will be used to test the accuracy of your model.\n",
        "\n",
        "The information about the images and keypoints in this dataset are summarized in CSV files, which we can read in using `pandas`. Let's read the training CSV and get the annotations in an (N, 2) array where N is the number of keypoints and 2 is the dimension of the keypoint coordinates (x, y).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbJ1oCak-oSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /data\n",
        "!wget -P /data/ https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5aea1b91_train-test-data/train-test-data.zip\n",
        "!unzip -n /data/train-test-data.zip -d /data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmWRxg1yZoZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's look in our data folder we just created to see what Udacity gives us.\n",
        "!ls -la /data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRv1nSsiWj4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the required libraries\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7suP-TuG_zeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set our working directory to /data\n",
        "import os\n",
        "os.chdir('/data')\n",
        "os.getcwd()\n",
        "os.listdir('.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wai8sakAQ0AY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_frames = pd.read_csv('/data/test_frames_keypoints.csv')\n",
        "test_frames.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8lMwxKQW1X3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the training data into a Pandas dataframe and view the first 5 rows of data\n",
        "key_pts_frame = pd.read_csv('/data/training_frames_keypoints.csv')\n",
        "print(key_pts_frame.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7xZcRE8XDya",
        "colab_type": "text"
      },
      "source": [
        "Let's look at a single image's shape and the facial keypoints associated with it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp8KCZFpW6ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = 0\n",
        "\n",
        "# What pandas function can I use to grab the image name out of the first column?\n",
        "image_name = key_pts_frame.iloc[n,0]\n",
        "\n",
        "# Grab the key_point values, convert them to a numpy.ndarray object. as_matrix() is deprecated.\n",
        "key_points = key_pts_frame.iloc[n,1:].values\n",
        "\n",
        "# convert the values to data type 'float' and give me a listing of the key points in [x,y] format.\n",
        "#key_pts = key_pts.astype('float').reshape(-1, 2)\n",
        "key_points = key_pts.astype('float').reshape(68, 2)\n",
        "\n",
        "\n",
        "print('Image name: ', image_name)\n",
        "print('Landmarks shape: ', key_points.shape)\n",
        "print('First 4 key pts: {}'.format(key_points[:4]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVXBC6qXcEVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many images do we have in the dataset? \n",
        "print('Number of images: ', len(key_pts_frame))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4FHOIk9dxcw",
        "colab_type": "text"
      },
      "source": [
        "## Resizing and Converting to Grayscale with Keras Image Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAc8C-bZat5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import load_img, img_to_array, save_img\n",
        "from numpy import expand_dims, asarray\n",
        "\n",
        "img = load_img(os.path.join('/data/training/', image_name), color_mode = 'grayscale', target_size=(224, 224))\n",
        "data = asarray(img)\n",
        "print(data.shape)\n",
        "# add in the grayscale channel\n",
        "print(\"Adding in the grayscale channel with the expand_dims function.\")\n",
        "data_last = expand_dims(data, axis=2)\n",
        "print(data_last.shape)\n",
        "print(type(img))\n",
        "print(img.size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_7c4f7VTcI6",
        "colab_type": "text"
      },
      "source": [
        "## Resizing and Converting to Grayscale with OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLOl1B-CDy6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_name = key_pts_frame.iloc[5,0]\n",
        "im = cv2.imread(os.path.join('/data/training/', image_name))\n",
        "im = cv2.resize(im, (224, 224))\n",
        "if (im.shape[2] == 4):\n",
        "  im = im[:,:,0:3]\n",
        "im = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "print(type(im))\n",
        "print(im.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Or2O10bTCxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import expand_dims, asarray\n",
        "im = expand_dims(im, axis=2)\n",
        "print(im.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-pkOCsojwRD",
        "colab_type": "text"
      },
      "source": [
        "Let's create a function to draw the keypoints on top of each image. This will be somewhat similar to what we did in the previous lesson, by using the matplotlib library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo2kQMqRjF4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_keypoints(image, key_pts):\n",
        "    \"\"\"Show image with keypoints\"\"\"\n",
        "    # first plot (show) the image using matplotlib\n",
        "    plt.imshow(image)\n",
        "    \n",
        "    # second, draw the key points on top of the plotted image using matplotlib\n",
        "    plt.scatter(key_pts[:, 0], key_pts[:, 1], marker='.', c='m')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHw_bPzZdh1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import save_img\n",
        "save_img('keras_gray_test.jpg', im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9f6_0s1fW_k",
        "colab_type": "text"
      },
      "source": [
        "Saving our new rescaled and greyscaled image. We are missing one step. See if you can spot what we missed doing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axo9KiOeequD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "im_gray = load_img('keras_gray_test.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOQ4_Gfgd4mX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab the key_point values, convert them to a numpy.ndarray object. as_matrix() is deprecated.\n",
        "key_points = key_pts_frame.iloc[5,1:].values\n",
        "\n",
        "# convert the values to data type 'float' and give me a listing of the key points in [x,y] format.\n",
        "#key_pts = key_pts.astype('float').reshape(-1, 2)\n",
        "key_points = key_points.astype('float').reshape(68, 2)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "show_keypoints(im_gray, key_points)\n",
        "plt.show()\n",
        "print(im_gray.size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KveFeGQPrIoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display a few different types of images by changing the index n\n",
        "\n",
        "# select an image by index in our data frame\n",
        "image_name = key_pts_frame.iloc[n, 0]\n",
        "image = mpimg.imread(os.path.join('/data/training/', image_name))\n",
        "if (image.shape[2] == 4):\n",
        "  image = image[:,:,0:3]\n",
        "sample = {'image': image, 'keypoints': key_pts}  \n",
        "train_df.append(sample)\n",
        "                     \n",
        "\n",
        "# Key_pts is our y\n",
        "key_pts = key_pts_frame.iloc[n, 1:].values\n",
        "key_pts = key_pts.astype('float').reshape(-1, 2)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "show_keypoints(mpimg.imread(os.path.join('/data/training/', image_name)), key_pts)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqGL2LZ_J68R",
        "colab_type": "text"
      },
      "source": [
        "## Transforms\n",
        "\n",
        "Now, the images above are not of the same size, and neural networks often expect images that are standardized; a fixed size, with a normalized range for color ranges and coordinates, and are tensors of float values.\n",
        "\n",
        "Therefore, we will need to write some pre-processing code.\n",
        "\n",
        "Let's create four transforms:\n",
        "\n",
        "-  ``Normalize``: to convert a color image to grayscale values with a range of [0,1] and normalize the keypoints to be in a range of about [-1, 1]\n",
        "-  ``Rescale``: to rescale an image to a desired size.\n",
        "-  ``RandomCrop``: to crop an image randomly.\n",
        "-  ``ToTensor``: to convert numpy images to tensor images.\n",
        "\n",
        "I recommend looking at these guides for help in doing these transformations in Keras.\n",
        "\n",
        "[How To Normalize Center and Standardize Images in Keras](https://machinelearningmastery.com/how-to-normalize-center-and-standardize-images-with-the-imagedatagenerator-in-keras/)\n",
        "\n",
        "[How To Load, Convert, and Save Images in Keras](https://machinelearningmastery.com/how-to-load-convert-and-save-images-with-the-keras-api/)\n",
        "\n",
        "[How To Use Pre-Trained VGG Model in Keras](https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/)\n",
        "\n",
        "\n",
        "Helpful code snippets from the above tutorials by Jason Brownlee are listed below:\n",
        "```\n",
        "# example of loading an image with the Keras API\n",
        "from keras.preprocessing.image import load_img\n",
        "# load the image\n",
        "img = load_img('bondi_beach.jpg')\n",
        "# report details about the image\n",
        "print(type(img))\n",
        "print(img.format)\n",
        "print(img.mode)\n",
        "print(img.size)\n",
        "# show the image\n",
        "img.show()\n",
        "```\n",
        "\n",
        "```\n",
        "# example of converting an image with the Keras API\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import array_to_img\n",
        "# load the image\n",
        "img = load_img('bondi_beach.jpg')\n",
        "print(type(img))\n",
        "# convert to numpy array\n",
        "img_array = img_to_array(img)\n",
        "print(img_array.dtype)\n",
        "print(img_array.shape)\n",
        "# convert back to image\n",
        "img_pil = array_to_img(img_array)\n",
        "print(type(img))\n",
        "```\n",
        "\n",
        "```\n",
        "# importing pre-trained model example\n",
        "from keras.applications.vgg16 import VGG16\n",
        "model = VGG16()\n",
        "print(model.summary())\n",
        "```\n",
        "\n",
        "\n",
        "Helpful code snippet from Udacity's project code\n",
        "```\n",
        "# convert image to grayscale\n",
        "image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "# scale color range from [0, 255] to [0, 1]\n",
        "image_copy=  image_copy/255.0\n",
        "\n",
        "# scale keypoints to be centered around 0 with a range of [-1, 1]\n",
        "# mean = 100, sqrt = 50, so, pts should be (pts - 100)/50\n",
        "key_pts_copy = (key_pts_copy - 100)/50.0\n",
        "```\n",
        "\n",
        "Observe some examples below how these transforms are generally applied to both the image and its keypoints.\n",
        "\n",
        "**these are one way to code these transforms, but not the ONLY way. There are many ways**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1KWgzU93DOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Normalize(object):\n",
        "    \"\"\"Convert a color image to grayscale and normalize the color range to [0,1].\"\"\"        \n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "        \n",
        "        image_copy = np.copy(image)\n",
        "        key_pts_copy = np.copy(key_pts)\n",
        "\n",
        "        # convert image to grayscale. Use image_copy\n",
        "        cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "        \n",
        "        # scale color range from [0, 255] to [0, 1]. Use image_copy\n",
        "        \n",
        "        \n",
        "        # scale keypoints to be centered around 0 with a range of [-1, 1]\n",
        "        # mean = 100, sqrt = 50, so, pts should be (pts - 100)/50\n",
        "        key_pts_copy = _______\n",
        "\n",
        "\n",
        "        return {'image': image_copy, 'keypoints': key_pts_copy}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6-78_6_3H2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "\n",
        "        img = cv2.resize(image, (new_w, new_h))\n",
        "        \n",
        "        # scale the pts, too\n",
        "        key_pts = key_pts * [new_w / w, new_h / h]\n",
        "\n",
        "        return {'image': img, 'keypoints': key_pts}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UJJZPdk3LNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomCrop(object):\n",
        "    \"\"\"Crop randomly the image in a sample.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If int, square crop\n",
        "            is made.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        if isinstance(output_size, int):\n",
        "            self.output_size = (output_size, output_size)\n",
        "        else:\n",
        "            assert len(output_size) == 2\n",
        "            self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, key_pts = sample['image'], sample['keypoints']\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        new_h, new_w = self.output_size\n",
        "\n",
        "        top = np.random.randint(0, h - new_h)\n",
        "        left = np.random.randint(0, w - new_w)\n",
        "\n",
        "        image = image[top: top + new_h,\n",
        "                      left: left + new_w]\n",
        "\n",
        "        key_pts = key_pts - [left, top]\n",
        "\n",
        "        return {'image': image, 'keypoints': key_pts}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaCuSw3vrS6q",
        "colab_type": "text"
      },
      "source": [
        "## Create Training Data Frame\n",
        "\n",
        "We have the data and we have csv files with the names of the images and their keypoints, but it's not in a format we can work with.\n",
        "\n",
        "Each image needs to be read in and the pixel values stored in one column representing our features (x).\n",
        "\n",
        "The Keypoints need to be reformatted as [x,y] pairs and stored in one column representing our target variable (y).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2ALn49TCoo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = []\n",
        "\n",
        "for n in range(3):\n",
        "  imName = key_pts_frame.iloc[n, 0]\n",
        "  print ('adding in image: ', imName)\n",
        "  image = mpimg.imread(os.path.join('/data/training/', imName))\n",
        "  \n",
        "  #if there are 4 channels, change it to having only 3\n",
        "  if (image.shape[2] == 4):\n",
        "    image = image[:,:,0:3]\n",
        "  \n",
        "  # Grab the keypoints values and reshape them as [x,y]\n",
        "  key_pts = key_pts_frame.iloc[n, 1:].values\n",
        "  key_pts = key_pts.astype('float').reshape(-1, 2)\n",
        "  \n",
        "  # Create a sample as a dictionary of image and keypoints. Append this to the samples list created above.\n",
        "  sample = {'image': image, 'keypoints': key_pts}  \n",
        "  samples.append(sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_IosqmhMkV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.DataFrame(samples)\n",
        "print(train_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu4j65REMqI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_df['image'][0].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBVe20HZSx_Q",
        "colab_type": "text"
      },
      "source": [
        "### Sample code below for using ImageDataGenerator to apply some transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAAYbL4YqRsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create and configure the data generator\n",
        "datagen = ImageDataGenerator()\n",
        "\n",
        "# calculate scaling statistics on the training dataset\n",
        "datagen.fit(trainX)\n",
        "\n",
        "# get batch iterator\n",
        "train_iterator = datagen.flow(trainX, trainy)\n",
        "\n",
        "# fit model\n",
        "model.fit_generator(train_iterator, ....)\n",
        "\n",
        "# get batch iterator for training\n",
        "train_iterator = datagen.flow(trainX, trainy)\n",
        "# get batch iterator for validation\n",
        "val_iterator = datagen.flow(valX, valy)\n",
        "# fit model\n",
        "model.fit_generator(train_iterator, validation_data=val_iterator, ...)\n",
        "\n",
        "# get batch iterator for testing\n",
        "test_iterator = datagen.flow(testX, testy)\n",
        "# evaluate model loss on test dataset\n",
        "loss = model.evaluate_generator(test_iterator, ...)\n",
        "\n",
        "# Normalization\n",
        "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "# prepare an iterators to scale images\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9m0R9nVzaS6",
        "colab_type": "text"
      },
      "source": [
        "## Sample Code: Apply The Transformations with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Leu-Plzgr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Using real-time data augmentation.')\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=True,  # set input mean to 0 over the dataset\n",
        "    featurewise_std_normalization=True,  # divide inputs by std of the dataset\n",
        "    rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    # randomly shift images horizontally (fraction of total width)\n",
        "    width_shift_range=0.1,\n",
        "    # randomly shift images vertically (fraction of total height)\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.,  # set range for random shear\n",
        "    zoom_range=0.,  # set range for random zoom\n",
        "    horizontal_flip=True,  # randomly flip images\n",
        "    # set rescaling factor (applied before any other transformation)\n",
        "    rescale=1.0/255.0,\n",
        "    # set function that will be applied on each input\n",
        "    preprocessing_function=None,\n",
        "    # image data format, either \"channels_first\" or \"channels_last\"\n",
        "    data_format=\"channels_last\",\n",
        "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "    validation_split=0.3)\n",
        "\n",
        "# Compute quantities required for feature-wise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(x_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "femD05nSzhTV",
        "colab_type": "text"
      },
      "source": [
        "## Create Model Architecture\n",
        "\n",
        "For this lesson, as a baseline, reproduce the model architecture and performance documented in this research paper:\n",
        "\n",
        "[NamishNet](https://arxiv.org/pdf/1710.00977.pdf)\n",
        "\n",
        "Recall that CNN's are defined by a few types of layers:\n",
        "\n",
        "* Convolutional layers\n",
        "* Maxpooling layers\n",
        "* Fully-connected layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9cTcNMW5EsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your neural network\n",
        "\n",
        "from keras import models \n",
        "from keras import layers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Conv layer\n",
        "model.add(Conv2D(32, (4,4), activation='elu', input_shape=(224,224,1)))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# 2nd Conv layer\n",
        "model.add(Conv2D(64, (3,3), activation='elu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# 3rd Conv layer\n",
        "model.add(Conv2D(128, (2,2), activation='elu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# 4th Conv layer\n",
        "model.add(Conv2D(256, (1,1), activation='elu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Flatten from 3D to 1D Tensor\n",
        "model.add(Flatten())\n",
        "\n",
        "# 1st fully connected (Dense) layer\n",
        "model.add(Dense(1000, kernel_initializer='glorot_uniform', activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# 2nd fully connected (Dense) layer\n",
        "model.add(Dense(1000, kernel_initializer='glorot_uniform', activation='relu'))\n",
        "model.add(Dropout(0.6))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
        "          \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk6Kff-t5Iui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile your neural network\n",
        "\n",
        "model.compile(loss='rmse', optimizer=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8), metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8sQMQjN5K9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train your neural network\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                 batch_size=batch_size),\n",
        "                    epochs=20,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAXz5Cl35i6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate your model on the test images\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y8nhXex5Ngv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (opt) Save a model with decent performance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lXtOtDD7GK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHQNcntp5tCD",
        "colab_type": "text"
      },
      "source": [
        "## Model Knowledge Check\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE9GzBlWyeaV",
        "colab_type": "text"
      },
      "source": [
        "### Question 1: What optimization and loss functions did you choose and why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aEkC3BnyeaX",
        "colab_type": "text"
      },
      "source": [
        "### Question 2: What kind of network architecture did you start with and how did it change as you tried different architectures? Did you decide to add more convolutional layers or any layers to avoid overfitting the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgfH_teKyeaZ",
        "colab_type": "text"
      },
      "source": [
        "### Question 3: How did you decide on the number of epochs and batch_size to train your model?"
      ]
    }
  ]
}