{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "linear_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinajordan/DS-Training/blob/master/linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS0myVsWhxqw",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression\n",
        "\n",
        "What is regression? What types of learning problems are good for regression?\n",
        "\n",
        "Short rule: If the target variable is **continuous** (varying at each observation), regression is a good task for solving it.\n",
        "\n",
        "Linear regression was developed in the field of statistics and is studied as a model for understanding the relationship between input and output numerical variables. It is a really important part of machine learning. \n",
        "\n",
        "Two basic forms of Linear Regression:\n",
        "* Simple Linear Regression (SLR) which deals with just two variables.\n",
        "* Multi-linear Regression (MLR) which deals with more than two variables\n",
        "\n",
        "Dataset: Boston Housing\n",
        "\n",
        "![alt text](https://image.ibb.co/jRH8E9/Capture.jpg)\n",
        "\n",
        "$X$ is the set of input values; $h$ is the function that maps the $X$ values to predicted $y$ (called y hat ($\\hat y$) or predictor). $h$ is often referred to as the hypothesis function.\n",
        "\n",
        "The ultimate goal is, given a training set, to learn a function $ h:X→Y$ so that h(x) is a \"good\" predictor for the corresponding value of y. \n",
        "\n",
        "A pair ($x^i, y^i$) is called a training example.\n",
        "\n",
        "Here's an example dataset to use for the next explanations.\n",
        "\n",
        "![alt text](https://image.ibb.co/hXTenU/Capture.jpg)\n",
        "\n",
        "The example dataset contains two features: $x_1^i$ is the living area of the i-th house in the training set, and $x_2^i$ is its number of bedrooms.\n",
        "\n",
        "To perform regression, you must decide the way you are going to represent h. As an initial choice, let’s say you decide to approximate y as a linear function of x:\n",
        "\n",
        "$h_θ(x) = θ_0 + θ_1x_1 + θ_2 x_2$\n",
        "\n",
        "Here, the $θ_i$’s are the parameters (also called weights) parameterizing the space of linear functions mapping from $X$ to $Y$. Essentially, these parameters are used for accurately mapping $X$ to $Y$.\n",
        "\n",
        "The above formula can be rewritten as \n",
        "\n",
        "![alt text](https://image.ibb.co/i94zMp/Capture.jpg)\n",
        "\n",
        "One prominent method seems to be to make $h(x)$ close to $y$, at least for the training examples you have. To understand this more formally, let's try defining a function that determines, for each value of the $θ$’s, how close the $h(x^i)$’s are to the corresponding $y^i$ ’s.\n",
        "\n",
        "\n",
        "*Note:The weight parameters are also called model coefficients.* \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xBpMZ1dNR0w",
        "colab_type": "text"
      },
      "source": [
        "## Cost function: \n",
        "\n",
        "The main question, is how do you pick or learn the parameters $θ$? You cannot change your input instances as to predict the prices. You have only these $θ$ parameters to tune/adjust.\n",
        "\n",
        "This brings up one of the most important functions in machine learning and data science. The cost function:\n",
        "\n",
        "![alt text](https://image.ibb.co/kW8e49/Capture.jpg)\n",
        "\n",
        "\n",
        "Learning/training a linear regression model essentially means estimating the values of the coefficients/parameters used in the representation with the data you have to minimize the cost function, $J(\\theta)$\n",
        "\n",
        "## Gradient Descent:\n",
        "\n",
        "![alt text](https://image.ibb.co/jUXWj9/Capture.jpg)\n",
        "\n",
        "Choose $\\theta$ so as to minimize $J(\\theta)$. To do so, let’s use a search algorithm that starts with some \"initial guess\" for $\\theta$, and that iteratively changes $\\theta$ to make $J(\\theta)$ smaller, until hopefully, you converge to a value of $\\theta$ that minimizes $J(\\theta)$.\n",
        "\n",
        "Here, $\\alpha$ is called the learning rate. This is a very natural algorithm that repeatedly takes a step in the direction of steepest decrease of $J$. This term $\\alpha$ effectively controls how steep your algorithm would move to the decrease of $J$. It can be pictorially expressed as the following:\n",
        "\n",
        "![alt text](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png)\n",
        "\n",
        "The term $\\alpha$ (learning rate) is very important here since it determines the size of the improvement step to take on each iteration of the procedure.\n",
        "\n",
        "Now there are commonly two variants of gradient descent:\n",
        "\n",
        "* The method that looks at every example in the entire training set on every step and is called batch gradient descent.\n",
        "* The method where you repeatedly run through the training set, and each time you encounter a training example, you update the parameters according to the gradient of the error with respect to that single training example only. This algorithm is called stochastic gradient descent (also incremental gradient descent).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRpKnuztNVYA",
        "colab_type": "text"
      },
      "source": [
        "## Regularization\n",
        "\n",
        "Generally, regularization methods work by penalizing the coefficients of features having extremely large values and thereby try to reduce the error. It not only results in an enhanced error rate but also, it reduces the model complexity. This is particularly very useful when you are dealing with a dataset that has a large number of features, and your baseline model is not able to distinguish between the importance of the features (not all features in a dataset are equally important, right?).\n",
        "\n",
        "\n",
        "Two variants of regularization for linear regression:\n",
        "\n",
        "**Lasso Regression** (aka L1 regularization) - adds a penalty term which is equivalent to the absolute value of the magnitude of the coefficients.\n",
        "\n",
        "![alt text](https://image.ibb.co/bzZ2cf/Capture.jpg)\n",
        "\n",
        "explained:\n",
        "* λ is the constant factor that you add in order to control the speed of the improvement in error (learning rate)\n",
        "* the dataset has (M+1) features, so it runs from 0 to M. wj is the weight/coefficient.\n",
        "\n",
        "\n",
        "**Ridge Regression** (aka L2 regularization) - adds a penalty term which is equivalent to the square of the magnitude of coefficients\n",
        "\n",
        "![alt text](https://image.ibb.co/hY4oiL/Capture.jpg)\n",
        "\n",
        "\n",
        "HIGHLY RECOMMENDED FURTHER READING:\n",
        "https://www.datacamp.com/community/tutorials/towards-preventing-overfitting-regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjLCGX7whsnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import dependencies and load dataset\n",
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "\n",
        "data = datasets.load_boston()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkJsTnd_8SJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "04b31e6e-55ea-4be8-96fb-2cca6f4ef666"
      },
      "source": [
        "# let's look at a description of our data\n",
        "print (data.DESCR)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _boston_dataset:\n",
            "\n",
            "Boston house prices dataset\n",
            "---------------------------\n",
            "\n",
            "**Data Set Characteristics:**  \n",
            "\n",
            "    :Number of Instances: 506 \n",
            "\n",
            "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
            "\n",
            "    :Attribute Information (in order):\n",
            "        - CRIM     per capita crime rate by town\n",
            "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
            "        - INDUS    proportion of non-retail business acres per town\n",
            "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
            "        - NOX      nitric oxides concentration (parts per 10 million)\n",
            "        - RM       average number of rooms per dwelling\n",
            "        - AGE      proportion of owner-occupied units built prior to 1940\n",
            "        - DIS      weighted distances to five Boston employment centres\n",
            "        - RAD      index of accessibility to radial highways\n",
            "        - TAX      full-value property-tax rate per $10,000\n",
            "        - PTRATIO  pupil-teacher ratio by town\n",
            "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
            "        - LSTAT    % lower status of the population\n",
            "        - MEDV     Median value of owner-occupied homes in $1000's\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
            "\n",
            "This is a copy of UCI ML housing dataset.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
            "\n",
            "\n",
            "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
            "\n",
            "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
            "prices and the demand for clean air', J. Environ. Economics & Management,\n",
            "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
            "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
            "pages 244-261 of the latter.\n",
            "\n",
            "The Boston house-price data has been used in many machine learning papers that address regression\n",
            "problems.   \n",
            "     \n",
            ".. topic:: References\n",
            "\n",
            "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
            "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TKX9i_l8W1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert our data to a pandas dataframe\n",
        "\n",
        "#Set the features\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "# set the target\n",
        "target = pd.DataFrame(data.target, columns=[\"MEDV\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oA8g4E1JH37",
        "colab_type": "text"
      },
      "source": [
        "# Easy Way - Sklearn  Implementation of Linear Regression\n",
        "\n",
        "simple example below. Just one feature will be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgNsErHXGh0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What sci-kit learn package implements linear regression\n",
        "from sklearn import _____\n",
        "x = df[\"RM\"]\n",
        "y = target[\"MEDV\"]\n",
        "\n",
        "lm = linear_model._____()\n",
        "model = lm.fit(x, y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpdM3g9kIIa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What function makes predictions off of your model.\n",
        "predictions = lm.____(x)\n",
        "print(predictions[0:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUbSnoJvIlXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What function gives you an accuracy score?\n",
        "lm.___(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flxuik7zI0MV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How can you look at your models coefficients?\n",
        "lm.coef_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdkhKFoyJty1",
        "colab_type": "text"
      },
      "source": [
        "## More Verbose with Statsmodels\n",
        "\n",
        "Single linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-zdDUpbJtO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df[\"RM\"]\n",
        "y = target[\"MEDV\"]\n",
        "\n",
        "# Fit and make the predictions by the model\n",
        "model = sm.OLS(y, X).fit()\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Print out the statistics\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJbt6LIfKEjB",
        "colab_type": "text"
      },
      "source": [
        "### Breaking down the output:\n",
        "\n",
        "* The first observation you should make here is you are using OLS method to train your linear regression model.\n",
        "\n",
        "\"The Ordinary Least Squares procedure seeks to minimize the sum of the squared residuals. This means that given a regression line through the data we calculate the distance from each data point to the regression line, square it, and sum all of the squared errors together. This is the quantity that ordinary least squares seeks to minimize.\" - Jason Brownlee\n",
        "\n",
        "* There's a value corresponding to R-Squared. R-squared is the “percent of variance explained” by the model. That is, R-squared is the fraction by which the variance of the errors is less than the variance of the dependent variable. R-squared values range from 0 to 1 and are commonly stated as percentages from 0% to 100%. R-squared will give you an estimate of the relationship between movements of a dependent variable based on an independent variable's movements. It doesn't tell you whether your chosen model is good or bad, nor will it tell you whether the data and predictions are biased. A high or low R-square isn't necessarily good or bad, as it doesn't convey the reliability of the model, nor whether you've chosen the right regression. You can get a low R-squared for a good model, or a high R-square for a poorly fitted model, and vice versa. \n",
        "\n",
        "* The coefficient (coef) of 3.634 means that if the RM variable increases by 1, the predicted value of MEDV increases by 3.634. \n",
        "\n",
        "\n",
        "* There is a 95% confidence intervals for the RM which means that the model predicts at a 95% percent confidence that the value of RM is between 3.548 to 3.759).\n",
        "\n",
        "Those are the most important points to keep in mind for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLXuyBHEKq9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets add a constant term to our model\n",
        "x = sm.add_constant(x)\n",
        "\n",
        "model = sm.OLS(y, x).fit()\n",
        "predictions = model.predict(x)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN6KdGyZK6ZQ",
        "colab_type": "text"
      },
      "source": [
        "**Breakdown of output:**\n",
        "* It can be clearly seen that the addition of the constant term has a direct effect on the coefficient term. Without the constant term, your model was passing through the origin, but now you have a y-intercept at -34.67. \n",
        "* Now the slope of the RM predictor is also changed from 3.634 to 9.1021 (coef of RM)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6wec88PLLZv",
        "colab_type": "text"
      },
      "source": [
        "### Multi-Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f1eAD1JLKlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets add a feature for our model.\n",
        "x = df[[\"RM\", \"LSTAT\"]]\n",
        "y = target[\"MEDV\"]\n",
        "\n",
        "model = sm.OLS(y, x).fit()\n",
        "predictions = model.predict(x)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLqW80_0LdHa",
        "colab_type": "text"
      },
      "source": [
        "### Breakdown of Output:\n",
        "* This model has a much higher R-squared value — 0.948, which essentially means that this model captures 94.8% of the variance in the dependent variable. Now, let's try to figure out the relationship between the two variables RM and LSTAT and median house value. \n",
        "* As RM increases by 1, MEDV will increase by 4.9069, and when LSTAT increases by 1, MEDV will decrease by 0.6557. This indicates that RM and LSTAT are statistically significant in predicting (or estimating) the median house value.\n",
        "\n",
        "Interpretation to tell to your boss:\n",
        "* Houses having a small number of rooms are likely to have low price values.\n",
        "* In the areas where the status of the population is lower, the house prices are likely to be low."
      ]
    }
  ]
}