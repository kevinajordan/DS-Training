{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinajordan/DS-Training/blob/master/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7EUNFrOBqIn",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "It is necessary to perform data pre-processing for every data science or machine learning project. Different algorithms make different assumptions about your data and require different transformations.\n",
        "\n",
        "You may have to make many different transformations and apply different algorithms to determine which representation and transformation works best at capturing the structure of your problem.\n",
        "\n",
        "In general there are 3 different transformations that you will come across often and have to do for different datasets.\n",
        "\n",
        "\n",
        "1.   Rescale\n",
        "2.   Standardize\n",
        "3.   Normalize\n",
        "\n",
        "\n",
        "We will go over each with the sci-kit learn library and the Pima Indians Diabetes Dataset below.\n",
        "\n",
        "You can read more about this dataset from here:\n",
        "\n",
        "https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
        "\n",
        "After covering these, we will cover how to handle missing data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzhjc2RIH1g3",
        "colab_type": "text"
      },
      "source": [
        "# Rescaling\n",
        "\n",
        "When your data has different features with different scales (min, max), it is beneficial to put them all on the same scale. This scale is often from 0 to 1.\n",
        "\n",
        "Helpful Reference:\n",
        "https://scikit-learn.org/stable/modules/preprocessing.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XHCJnoH7WeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import software dependencies\n",
        "# what sci-kit learn library performs rescaling of features to a specific range?\n",
        "import pandas as pd, numpy as np, scipy\n",
        "from sklearn.preprocessing import ________"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXOWxPo4Hnjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# grab the pima indians diabetes dataset\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eheuQ71_JOhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the CSV file using pandas and take a quick look at the first few lines\n",
        "df = ______\n",
        "\n",
        "df._____(n=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2La1u8rNp-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the CSV file again with the appended data.\n",
        "feature_names = _______\n",
        "\n",
        "df = ________"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXYXf9WVI5BF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# store the values of the data frame in a variable named array\n",
        "array = df._____"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmQ9uHqTTdBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separate array into input and output components\n",
        "X = array[__, ___]\n",
        "Y = array[___, ___]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saU8enY2TtIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply the sci-kit learn function that performs rescaling\n",
        "scaler = ______(feature_range=(0, 1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7m7z5k4Zw9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What sci-kit learn function applies the scaled range to fit to the training data and transforms it?\n",
        "rescaledX = scaler._____(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDtxLZGmbAu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize transformed data\n",
        "np.set_printoptions(precision=3)\n",
        "print(rescaledX[0:5,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-6VmkLebCg3",
        "colab_type": "text"
      },
      "source": [
        "# Standardize\n",
        "\n",
        "Standardization is a useful technique to transform attributes with a Gaussian (i.e. normal) distribution and differing means and standard deviations to a standard Gaussian distribution with a **mean of 0 and a standard deviation of 1**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv8_yEwncqu2",
        "colab_type": "text"
      },
      "source": [
        "Repeat the steps to read the data into a dataframe and create separate arrays for input and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lc6yMEBbEtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What sci-kit learn function standardizes your data?\n",
        "scaler = _______(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVCwdG0Tb4-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the mean and std to your training data\n",
        "rescaledX = scaler.____(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvtFYZyacleE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize transformed data\n",
        "np.set_printoptions(precision=3)\n",
        "print(rescaledX[0:5,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEGqtuE0cufS",
        "colab_type": "text"
      },
      "source": [
        "# Normalize\n",
        "\n",
        "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called a unit norm in linear algebra).\n",
        "\n",
        "This preprocessing can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using algorithms that weight input values such as neural networks and algorithms that use distance measures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C7Dl1DHIc_r6"
      },
      "source": [
        "Repeat the steps to read the data into a dataframe and create separate arrays for input and output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEuYKi1sc6ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What sci-kit learn function normalizes the data?\n",
        "scaler = _______().fit(X)\n",
        "normalizedX = scaler.transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61vQWHMxdRwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarize transformed data\n",
        "np.set_printoptions(precision=3)\n",
        "print(rescaledX[0:5,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2WFqGaDdxFK",
        "colab_type": "text"
      },
      "source": [
        "Take 10 min to read more about the importance of re-scaling here:\n",
        "https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAWINrcZe0uK",
        "colab_type": "text"
      },
      "source": [
        "# Handling Missing Data\n",
        "\n",
        "We will continue to use the Pima Indians Diabetes Dataset as there is missing data within it.\n",
        "\n",
        "We will go over the following methods:\n",
        "\n",
        "\n",
        "*   Removing rows with missing values\n",
        "*   Imputing missing values\n",
        "\n",
        "Lastly, we will cover Algorithms that can handle missing values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DkaoMg6fnuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the Pima Indians Diabetes dataset without the header\n",
        "df = _____"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyG7_5Kzf1bJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print out the summary statistics for the dataset? Do you notice any columns that have 0 as the min that shouldn't?\n",
        "print(df.____)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHbRuw2-gkcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print out a summary of all the columns with zeros\n",
        "print((df____ == 0).sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USmHQJ1FhjBD",
        "colab_type": "text"
      },
      "source": [
        "In Python, mark missing values as NaN in order for you to drop these rows later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhQG1xTVh4uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What pandas functions replaces the 0 with a numpy NaN?\n",
        "df[[___]] = df[[_____]].____()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PcG95h-iKF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What pandas functions drops rows with missing data or Na values?\n",
        "df.____(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2zHmt1-iYag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# What is the shape of our dataset now? Has it been reduced?\n",
        "print(df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-7tpkBsinY4",
        "colab_type": "text"
      },
      "source": [
        "## Imputing Missing Values\n",
        "Imputing refers to using a model to replace missing values.\n",
        "\n",
        "There are many options we could consider when replacing a missing value, for example:\n",
        "\n",
        "*  A constant value that has meaning within the domain, such as 0, distinct from all other values.\n",
        "*  A value from another randomly selected record.\n",
        "*  A mean, median or mode value for the column.\n",
        "*  A value estimated by another predictive model.\n",
        "\n",
        "Any imputing performed on the training dataset will have to be performed on new data in the future when predictions are needed from the finalized model. This needs to be taken into consideration when choosing how to impute the missing values.\n",
        "\n",
        "For example, if you choose to impute with mean column values, these mean column values will need to be stored to file for later use on new data that has missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYX4cmCOjR1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mark zero values as missing or NaN\n",
        "dataset[[1,2,3,4,5]] = dataset[[1,2,3,4,5]].replace(0, numpy.NaN)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9HPE2hjqL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fill missing values with mean column values\n",
        "dataset.fillna(dataset.mean(), inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH8hn6hRjrs3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# count the number of NaN values in each column\n",
        "print(dataset.isnull().sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJz_uN5yj8nv",
        "colab_type": "text"
      },
      "source": [
        "Is there an impute function in the sci-kit learn library? Apply it to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shMUk4rOjsvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = read_csv('pima-indians-diabetes.csv', header=None)\n",
        "# mark zero values as missing or NaN\n",
        "df[[___]] = df[[_____]].____()\n",
        "# split dataset into inputs and outputs\n",
        "values = df.values\n",
        "X = values[:,0:8]\n",
        "y = values[:,8]\n",
        "# fill missing values with mean column values\n",
        "imputer = _______()\n",
        "transformed_X = imputer.fit_transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "107n-YdrkpXA",
        "colab_type": "text"
      },
      "source": [
        "Take 10 min to read this documentation.\n",
        "\n",
        "https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CSxU6oCk0Ec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}