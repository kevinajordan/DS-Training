{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19_keras_text_generation_lstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevinajordan/DS-Training/blob/master/19_keras_text_generation_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFCVSRaIFEaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Activation\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck5QYA3X4u_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.gutenberg.org/files/11/11-0.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVEgtUL34yCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv 11-0.txt wonderland.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XII3Z9Kt3xJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0vDN2dQGA7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!# load ascii text and covert to lowercase\n",
        "filename = \"wonderland.txt\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMSrc-xA5gcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read in the file with open() and convert to lowercase\n",
        "raw_text = _______\n",
        "raw_text = ______"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu3AzXVM0XlD",
        "colab_type": "text"
      },
      "source": [
        "<!--\n",
        "raw_text = raw_text[624:-18765]\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOxVn0uF_4VK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Strip out the Project Gutenberg Header and Footer\n",
        "raw_text = __________"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDoh58Gd4jMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZcbPhijGHkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (char_to_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-KsZuebHc8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: \", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvpfy8npGOEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "# input chars list\n",
        "dataX = []\n",
        "\n",
        "# output chars\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVMsVQdeGrSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz82BGC2Gy8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuUbUAw5G0gN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDeW8DDaC55N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoYAf88WG_9I",
        "colab_type": "text"
      },
      "source": [
        "First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
        "\n",
        "Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
        "\n",
        "Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector with a length of 47, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents.\n",
        "\n",
        "### Reference\n",
        "Today's practical is pulled from Jason Brownlee's Machine Learning Mastery:\n",
        "\n",
        "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
      ]
    }
  ]
}